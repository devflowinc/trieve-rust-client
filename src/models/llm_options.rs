/*
 * Trieve API
 *
 * Trieve OpenAPI Specification. This document describes all of the operations available through the Trieve API.
 *
 * The version of the OpenAPI document: 0.11.7
 * Contact: developers@trieve.ai
 * Generated by: https://openapi-generator.tech
 */

use crate::models;

/// LlmOptions : LLM options to use for the completion. If not specified, this defaults to the dataset's LLM options.
#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct LlmOptions {
    /// Completion first decides whether the stream should contain the stream of the completion response or the chunks first. Default is false. Keep in mind that || is used to separate the chunks from the completion response. If || is in the completion then you may want to split on ||{ instead.
    #[serde(rename = "completion_first", default, with = "::serde_with::rust::double_option", skip_serializing_if = "Option::is_none")]
    pub completion_first: Option<Option<bool>>,
    /// Frequency penalty is a number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. Default is 0.7.
    #[serde(rename = "frequency_penalty", default, with = "::serde_with::rust::double_option", skip_serializing_if = "Option::is_none")]
    pub frequency_penalty: Option<Option<f32>>,
    /// The maximum number of tokens to generate in the chat completion. Default is None.
    #[serde(rename = "max_tokens", default, with = "::serde_with::rust::double_option", skip_serializing_if = "Option::is_none")]
    pub max_tokens: Option<Option<i32>>,
    /// Presence penalty is a number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Default is 0.7.
    #[serde(rename = "presence_penalty", default, with = "::serde_with::rust::double_option", skip_serializing_if = "Option::is_none")]
    pub presence_penalty: Option<Option<f32>>,
    /// Stop tokens are up to 4 sequences where the API will stop generating further tokens. Default is None.
    #[serde(rename = "stop_tokens", default, with = "::serde_with::rust::double_option", skip_serializing_if = "Option::is_none")]
    pub stop_tokens: Option<Option<Vec<String>>>,
    /// Whether or not to stream the response. If this is set to true or not included, the response will be a stream. If this is set to false, the response will be a normal JSON response. Default is true.
    #[serde(rename = "stream_response", default, with = "::serde_with::rust::double_option", skip_serializing_if = "Option::is_none")]
    pub stream_response: Option<Option<bool>>,
    /// Optionally, override the system prompt in dataset server settings.
    #[serde(rename = "system_prompt", default, with = "::serde_with::rust::double_option", skip_serializing_if = "Option::is_none")]
    pub system_prompt: Option<Option<String>>,
    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Default is 0.5.
    #[serde(rename = "temperature", default, with = "::serde_with::rust::double_option", skip_serializing_if = "Option::is_none")]
    pub temperature: Option<Option<f32>>,
}

impl LlmOptions {
    /// LLM options to use for the completion. If not specified, this defaults to the dataset's LLM options.
    pub fn new() -> LlmOptions {
        LlmOptions {
            completion_first: None,
            frequency_penalty: None,
            max_tokens: None,
            presence_penalty: None,
            stop_tokens: None,
            stream_response: None,
            system_prompt: None,
            temperature: None,
        }
    }
}

